### 1、代价函数

- [![J(\theta ) = \frac{1}{{2{\text{m}}}}\sum\limits_{i = 1}^m {{{({h_\theta }({x^{(i)}}) - {y^{(i)}})}^2}} ](https://camo.githubusercontent.com/d3e70af1f44e4ac8ebb395c19564bf4606b5c94adac36107ccab73d9c64814a5/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d4a2532382535437468657461253230253239253230253344253230253543667261632537423125374425374225374232253742253543746578742537426d25374425374425374425374425354373756d2535436c696d6974735f25374269253230253344253230312537442535456d253230253742253742253742253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d253230253742792535452537422532386925323925374425374425323925374425354532253744253744253230)](https://camo.githubusercontent.com/d3e70af1f44e4ac8ebb395c19564bf4606b5c94adac36107ccab73d9c64814a5/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d4a2532382535437468657461253230253239253230253344253230253543667261632537423125374425374225374232253742253543746578742537426d25374425374425374425374425354373756d2535436c696d6974735f25374269253230253344253230312537442535456d253230253742253742253742253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d253230253742792535452537422532386925323925374425374425323925374425354532253744253744253230)
- 其中： [![{h_\theta }(x) = {\theta _0} + {\theta _1}{x_1} + {\theta _2}{x_2} + ...](https://camo.githubusercontent.com/c869f3094ef527094269b6da41b1ce6d9c1df4f42ae16aa9a8544c881a7f9fd1/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742685f25354374686574612532302537442532387825323925323025334425323025374225354374686574612532305f3025374425323025324225323025374225354374686574612532305f31253744253742785f3125374425323025324225323025374225354374686574612532305f32253744253742785f322537442532302532422532302e2e2e)](https://camo.githubusercontent.com/c869f3094ef527094269b6da41b1ce6d9c1df4f42ae16aa9a8544c881a7f9fd1/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742685f25354374686574612532302537442532387825323925323025334425323025374225354374686574612532305f3025374425323025324225323025374225354374686574612532305f31253744253742785f3125374425323025324225323025374225354374686574612532305f32253744253742785f322537442532302532422532302e2e2e)
- 下面就是要求出theta，使代价最小，即代表我们拟合出来的方程距离真实值最近
- 共有m条数据，其中[![{{{({h_\theta }({x^{(i)}}) - {y^{(i)}})}^2}}](https://camo.githubusercontent.com/faa4e6d0a093844c2261ccd8f72e36cb22e3e9a4d71b027f20ae942210daa403/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742253742253742253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d253230253742792535452537422532386925323925374425374425323925374425354532253744253744)](https://camo.githubusercontent.com/faa4e6d0a093844c2261ccd8f72e36cb22e3e9a4d71b027f20ae942210daa403/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742253742253742253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d253230253742792535452537422532386925323925374425374425323925374425354532253744253744)代表我们要拟合出来的方程到真实值距离的平方，平方的原因是因为可能有负值，正负可能会抵消
- 前面有系数`2`的原因是下面求梯度是对每个变量求偏导，`2`可以消去
- 实现代码：

```
# 计算代价函数
def computerCost(X,y,theta):
    m = len(y)
    J = 0
    
    J = (np.transpose(X*theta-y))*(X*theta-y)/(2*m) #计算代价J
    return J
```

- 注意这里的X是真实数据前加了一列1，因为有theta(0)

### 2、梯度下降算法

- 代价函数对[![{{\theta _j}}](https://camo.githubusercontent.com/ff246495c48dde2e5e09b065915636103e811ca38d407808e4b961035f550fc3/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d25374225374225354374686574612532305f6a253744253744)](https://camo.githubusercontent.com/ff246495c48dde2e5e09b065915636103e811ca38d407808e4b961035f550fc3/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d25374225374225354374686574612532305f6a253744253744)求偏导得到：
  ![\frac{{\partial J(\theta )}}{{\partial {\theta j}}} = \frac{1}{m}\sum\limits{i = 1}^m {[({h_\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](https://camo.githubusercontent.com/74ce9acf7541cf94f063dbc535bd389bbf931f2a808b6c66f96d1313ff0ab344/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253543667261632537422537422535437061727469616c2532304a25323825354374686574612532302532392537442537442537422537422535437061727469616c25323025374225354374686574612532305f6a25374425374425374425323025334425323025354366726163253742312537442537426d25374425354373756d2535436c696d6974735f25374269253230253344253230312537442535456d253230253742253542253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d2532302537427925354525374225323869253239253744253744253239785f6a25354525374225323869253239253744253544253744253230)
- 所以对theta的更新可以写为：
  ![{\theta j} = {\theta j} - \alpha \frac{1}{m}\sum\limits{i = 1}^m {[({h\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](https://camo.githubusercontent.com/5e15c6d82ce541b23a6f855ad105b5e6018b82566f686ebf29745d4ff3f2b01c/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d25374225354374686574612532305f6a25374425323025334425323025374225354374686574612532305f6a2537442532302d253230253543616c70686125323025354366726163253742312537442537426d25374425354373756d2535436c696d6974735f25374269253230253344253230312537442535456d253230253742253542253238253742685f253543746865746125323025374425323825374278253545253742253238692532392537442537442532392532302d2532302537427925354525374225323869253239253744253744253239785f6a25354525374225323869253239253744253544253744253230)
- 其中[![\alpha ](https://camo.githubusercontent.com/20814bb66fa8f84734bc7ad8a7b50116f8c8bb1c61c8a5892de42de7d5458155/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253543616c706861253230)](https://camo.githubusercontent.com/20814bb66fa8f84734bc7ad8a7b50116f8c8bb1c61c8a5892de42de7d5458155/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253543616c706861253230)为学习速率，控制梯度下降的速度，一般取**0.01,0.03,0.1,0.3.....**
- 为什么梯度下降可以逐步减小代价函数
- 假设函数`f(x)`
- 泰勒展开：`f(x+△x)=f(x)+f'(x)*△x+o(△x)`
- 令：`△x=-α*f'(x)` ,即负梯度方向乘以一个很小的步长`α`
- 将`△x`代入泰勒展开式中：`f(x+△x)=f(x)-α*[f'(x)]²+o(△x)`
- 可以看出，`α`是取得很小的正数，`[f'(x)]²`也是正数，所以可以得出：`f(x+△x)<=f(x)`
- 所以沿着**负梯度**放下，函数在减小，多维情况一样。
- 实现代码

```
# 梯度下降算法
def gradientDescent(X,y,theta,alpha,num_iters):
    m = len(y)      
    n = len(theta)
    
    temp = np.matrix(np.zeros((n,num_iters)))   # 暂存每次迭代计算的theta，转化为矩阵形式
    
    
    J_history = np.zeros((num_iters,1)) #记录每次迭代计算的代价值
    
    for i in range(num_iters):  # 遍历迭代次数    
        h = np.dot(X,theta)     # 计算内积，matrix可以直接乘
        temp[:,i] = theta - ((alpha/m)*(np.dot(np.transpose(X),h-y)))   #梯度的计算
        theta = temp[:,i]
        J_history[i] = computerCost(X,y,theta)      #调用计算代价函数
        print '.',      
    return theta,J_history  
```

### 3、均值归一化

- 目的是使数据都缩放到一个范围内，便于使用梯度下降算法
- [![{x_i} = \frac{{{x_i} - {\mu _i}}}{{{s_i}}}](https://camo.githubusercontent.com/9427968ae526e6c2308b38202a1d3b6a17387655f6b0a340a794f2cdff6eb6da/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742785f6925374425323025334425323025354366726163253742253742253742785f692537442532302d2532302537422535436d752532305f69253744253744253744253742253742253742735f69253744253744253744)](https://camo.githubusercontent.com/9427968ae526e6c2308b38202a1d3b6a17387655f6b0a340a794f2cdff6eb6da/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742785f6925374425323025334425323025354366726163253742253742253742785f692537442532302d2532302537422535436d752532305f69253744253744253744253742253742253742735f69253744253744253744)
- 其中 [![{{\mu _i}}](https://camo.githubusercontent.com/301daffc0f4abbe76f7bec1cd364867c4a23607b4012e5dcf7c716b5af8f4edd/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d2537422537422535436d752532305f69253744253744)](https://camo.githubusercontent.com/301daffc0f4abbe76f7bec1cd364867c4a23607b4012e5dcf7c716b5af8f4edd/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d2537422537422535436d752532305f69253744253744) 为所有此feture数据的平均值
- [![{{s_i}}](https://camo.githubusercontent.com/90d1f258fa5b370258758f96660b540e2184ea5f7f7d92b7213e8866c6b7af2a/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742253742735f69253744253744)](https://camo.githubusercontent.com/90d1f258fa5b370258758f96660b540e2184ea5f7f7d92b7213e8866c6b7af2a/687474703a2f2f63686172742e617069732e676f6f676c652e636f6d2f63686172743f6368743d7478266368733d317830266368663d62672c732c4646464646463030266368636f3d3030303030302663686c3d253742253742735f69253744253744)可以是**最大值-最小值**，也可以是这个feature对应的数据的**标准差**
- 实现代码：

```
# 归一化feature
def featureNormaliza(X):
    X_norm = np.array(X)            #将X转化为numpy数组对象，才可以进行矩阵的运算
    #定义所需变量
    mu = np.zeros((1,X.shape[1]))   
    sigma = np.zeros((1,X.shape[1]))
    
    mu = np.mean(X_norm,0)          # 求每一列的平均值（0指定为列，1代表行）
    sigma = np.std(X_norm,0)        # 求每一列的标准差
    for i in range(X.shape[1]):     # 遍历列
        X_norm[:,i] = (X_norm[:,i]-mu[i])/sigma[i]  # 归一化
    
    return X_norm,mu,sigma
```

- 注意预测的时候也需要均值归一化数据

## 因子分析(factor analysis)

是指研究从变量群中提取共性因子的统计技术。 因子分析是简化、分析高维数据的一种统计方法。

因子分析又存在两个方向，一个是探索性因子分析（exploratory factor analysis）。另一个是验证性因子分析（confirmatory factor analysis）。

**探索性因子分析**是先不假定一堆自变量背后到底有几个因子以及关系，而是我们通过这个方法去寻找因子及关系。

**验证性因子分析**是假设一堆自变量背后有几个因子，试图验证这种假设是否正确。

因子分析有两个核心问题，一是如何构造因子变量，二是如何对因子变量进行命名解释。

## 因子分析的一般步骤

1. 将原始数据标准化处理 X
2. 计算相关矩阵C
3. 计算相关矩阵C的特征值 r 和特征向量 U
4. 确定公共因子个数k
5. 构造初始因子载荷矩阵,其中U为r的特征向量
6. 建立因子模型
7. 对初始因子载荷矩阵A进行旋转变换，旋转变换是使初始因子载荷矩阵结构简化，关系明确，使得因子变量更具有可解释性，如果初始因子不相关，可以用方差极大正交旋转，如果初始因子间相关，可以用斜交旋转，进过旋转后得到比较理想的新的因子载荷矩阵A'.
8. 将因子表示成变量的线性组合，其中的系数可以通过最小二乘法得到.
9. 计算因子得分.

## 使用Python实现因子分析

### 0. 初始化构建数据

In [144]:

```
#导入库
import numpy as np
import pandas as pd
```

In [145]:

```
#构建数据集
data=pd.DataFrame(np.random.randint(50,100,size=(5, 10)))
data.columns=["特征1","特征2","特征3","特征4","特征5","特征6","特征7","特征8","特征9","特征10"]
data.index=['对象1','对象2','对象3','对象4','对象5']
```

In [146]:

```
#查看数据
data.head(3)
```

Out[146]:

|       | 特征1 | 特征2 | 特征3 | 特征4 | 特征5 | 特征6 | 特征7 | 特征8 | 特征9 | 特征10 |
| :---- | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | -----: |
| 对象1 |    62 |    89 |    52 |    63 |    84 |    51 |    67 |    62 |    59 |     56 |
| 对象2 |    50 |    98 |    82 |    85 |    61 |    97 |    63 |    96 |    88 |     59 |
| 对象3 |    52 |    54 |    63 |    94 |    93 |    62 |    88 |    75 |    60 |     61 |

### 1. 将原始数据标准化处理 X

In [147]:

```
data=(data-data.mean())/data.std() # 0均值规范化
```

In [148]:

```
data
```

Out[148]:

|       |    特征1 |     特征2 |     特征3 |     特征4 |     特征5 |     特征6 |     特征7 |     特征8 |     特征9 |    特征10 |
| :---- | -------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: |
| 对象1 | -0.03371 |  0.688104 | -1.140585 | -1.661849 |  0.805243 | -1.042202 | -0.706947 | -1.092949 | -0.555162 | -1.161295 |
| 对象2 | -1.04501 |  1.179606 |  0.623206 | -0.187628 | -0.536829 |  1.651129 | -1.007775 |  1.200895 |  1.680906 | -0.633434 |
| 对象3 | -0.87646 | -1.223295 | -0.493861 |  0.415462 |  1.330402 | -0.398144 |  0.872402 | -0.215891 | -0.478056 | -0.281526 |
| 对象4 |  1.31469 |  0.141990 | -0.376275 |  0.683502 | -0.886935 | -0.163942 |  1.248438 |  0.863565 | -0.786479 |  0.950151 |
| 对象5 |  0.64049 | -0.786404 |  1.387515 |  0.750512 | -0.711882 | -0.046841 | -0.406118 | -0.755619 |  0.138790 |  1.126105 |

### 2. 计算相关矩阵C

In [149]:

```
C=data.corr() #相关系数矩阵
```

In [150]:

```
C
```

Out[150]:

|        |     特征1 |     特征2 |     特征3 |     特征4 |     特征5 |     特征6 |     特征7 |     特征8 |     特征9 |    特征10 |
| :----- | --------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: | --------: |
| 特征1  |  1.000000 | -0.125185 |  0.053512 |  0.316812 | -0.563549 | -0.396722 |  0.423384 | -0.094383 | -0.565984 |  0.729563 |
| 特征2  | -0.125185 |  1.000000 | -0.147535 | -0.591560 | -0.318185 |  0.432786 | -0.561449 |  0.411365 |  0.491196 | -0.488141 |
| 特征3  |  0.053512 | -0.147535 |  1.000000 |  0.589383 | -0.641013 |  0.602759 | -0.321454 |  0.182064 |  0.601340 |  0.568450 |
| 特征4  |  0.316812 | -0.591560 |  0.589383 |  1.000000 | -0.456309 |  0.277390 |  0.568722 |  0.381112 | -0.006200 |  0.856592 |
| 特征5  | -0.563549 | -0.318185 | -0.641013 | -0.456309 |  1.000000 | -0.519135 |  0.078552 | -0.510000 | -0.346663 | -0.653499 |
| 特征6  | -0.396722 |  0.432786 |  0.602759 |  0.277390 | -0.519135 |  1.000000 | -0.365044 |  0.775420 |  0.916689 |  0.016999 |
| 特征7  |  0.423384 | -0.561449 | -0.321454 |  0.568722 |  0.078552 | -0.365044 |  1.000000 |  0.189765 | -0.689200 |  0.485650 |
| 特征8  | -0.094383 |  0.411365 |  0.182064 |  0.381112 | -0.510000 |  0.775420 |  0.189765 |  1.000000 |  0.486129 |  0.134735 |
| 特征9  | -0.565984 |  0.491196 |  0.601340 | -0.006200 | -0.346663 |  0.916689 | -0.689200 |  0.486129 |  1.000000 | -0.219108 |
| 特征10 |  0.729563 | -0.488141 |  0.568450 |  0.856592 | -0.653499 |  0.016999 |  0.485650 |  0.134735 | -0.219108 |  1.000000 |

### 3. 计算相关矩阵C的特征值 r 和特征向量 U

In [151]:

```
import numpy.linalg as nlg #导入nlg函数，linalg=linear+algebra

eig_value,eig_vector=nlg.eig(C) #计算特征值和特征向量

eig=pd.DataFrame() #利用变量名和特征值建立一个数据框

eig['names']=data.columns#列名

eig['eig_value']=eig_value#特征值
```

In [152]:

```
eig
```

Out[152]:

|      |  names |                                         eig_value |
| :--- | -----: | ------------------------------------------------: |
| 0    |  特征1 |                           (3.6178767303359534+0j) |
| 1    |  特征2 |                           (3.7848055707590635+0j) |
| 2    |  特征3 |                           (1.3573249428605407+0j) |
| 3    |  特征4 |                           (1.2399927560444444+0j) |
| 4    |  特征5 |  (2.1503169629143618e-16+1.1037482747695011e-16j) |
| 5    |  特征6 |  (2.1503169629143618e-16-1.1037482747695011e-16j) |
| 6    |  特征7 |                        (4.238056948937482e-17+0j) |
| 7    |  特征8 | (-2.7064622349968394e-16+2.5829577785548796e-17j) |
| 8    |  特征9 | (-2.7064622349968394e-16-2.5829577785548796e-17j) |
| 9    | 特征10 |                      (-1.6282653497481437e-16+0j) |

### 4. 确定公共因子个数k

In [153]:

```
from math import sqrt

for k in range(1,11): #确定公共因子个数
    if eig['eig_value'][:k].sum()/eig['eig_value'].sum()>=0.8: #如果解释度达到80%, 结束循环
        print(k)
        break
3
```

In [155]:

```
eig['eig_value'][:3].sum()/eig['eig_value'].sum()
```

Out[155]:

```
(0.8760007243955555+0j)
```

### 5. 构造初始因子载荷矩阵A

In [156]:

```
col0=list(sqrt(eig_value[0])*eig_vector[:,0]) #因子载荷矩阵第1列
col1=list(sqrt(eig_value[1])*eig_vector[:,1]) #因子载荷矩阵第2列
col2=list(sqrt(eig_value[2])*eig_vector[:,2]) #因子载荷矩阵第3列
A=pd.DataFrame([col0,col1,col2]).T #构造因子载荷矩阵A
A.columns=['factor1','factor2','factor3'] #因子载荷矩阵A的公共因子
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: ComplexWarning: Casting complex values to real discards the imaginary part
  """Entry point for launching an IPython kernel.
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part
  
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part
  This is separate from the ipykernel package so we can avoid doing imports until
```

In [157]:

```
A
```

Out[157]:

|      |                   factor1 |                   factor2 |                   factor3 |
| :--- | ------------------------: | ------------------------: | ------------------------: |
| 0    |   (0.7258972871759931+0j) |    (0.195764810516731+0j) |   (0.6134162515864857+0j) |
| 1    |     (-0.5879677931131+0j) | (-0.45462634230687354+0j) |   (0.6651556380337422+0j) |
| 2    |     (0.39128490248412+0j) |  (-0.7286311172822467+0j) | (-0.32426890310714734+0j) |
| 3    |   (0.8700578532724493+0j) | (-0.30144478238954536+0j) |  (-0.3458116643591109+0j) |
| 4    |  (-0.5069962398429965+0j) |   (0.6991573435745143+0j) | (-0.46872779482399285+0j) |
| 5    | (-0.10911519738585919+0j) |  (-0.9628827963582759+0j) | (-0.12683028873166624+0j) |
| 6    |   (0.6756467356889502+0j) |  (0.43522130228334177+0j) | (-0.01372604304275851+0j) |
| 7    |  (0.12912127991906552+0j) |  (-0.6937402312061534+0j) |  (0.18441987041279836+0j) |
| 8    | (-0.38573863856153534+0j) |  (-0.8962626145844621+0j) | (-0.20757371535363026+0j) |
| 9    |   (0.9717112549236306+0j) | (-0.16757177998509953+0j) | (0.028280482071635905+0j) |

### 6. 建立因子模型

In [158]:

```
h=np.zeros(10) #变量共同度，反映变量对共同因子的依赖程度，越接近1，说明公共因子解释程度越高，因子分析效果越好

D=np.mat(np.eye(10))#特殊因子方差，因子的方差贡献度 ，反映公共因子对变量的贡献，衡量公共因子的相对重要性

A=np.mat(A) #将因子载荷阵A矩阵化
```

In [159]:

```
for i in range(10):
    a=A[i,:]*A[i,:].T #A的元的行平方和
    h[i]=a[0,0]  #计算变量X共同度,描述全部公共因子F对变量X_i的总方差所做的贡献，及变量X_i方差中能够被全体因子解释的部分
    D[i,i]=1-a[0,0] #因为自变量矩阵已经标准化后的方差为1，即Var(X_i)=第i个共同度h_i + 第i个特殊因子方差
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part
  This is separate from the ipykernel package so we can avoid doing imports until
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: ComplexWarning: Casting complex values to real discards the imaginary part
  after removing the cwd from sys.path.
```

### 7. 对初始因子载荷矩阵A进行旋转变换.

### 8. 将因子表示成变量的线性组合.

In [160]:

```
from numpy import eye, asarray, dot, sum, diag #导入eye,asarray,dot,sum,diag 函数
from numpy.linalg import svd #导入奇异值分解函数

def varimax(Phi, gamma = 1.0, q =10, tol = 1e-6): #定义方差最大旋转函数
    p,k = Phi.shape #给出矩阵Phi的总行数，总列数
    R = eye(k) #给定一个k*k的单位矩阵
    d=0
    for i in range(q):
        d_old = d
        Lambda = dot(Phi, R)#矩阵乘法

        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, diag(diag(dot(Lambda.T,Lambda)))))) #奇异值分解svd

        R = dot(u,vh)#构造正交矩阵R

        d = sum(s)#奇异值求和

    if d_old!=0 and d/d_old:
        return dot(Phi, R)#返回旋转矩阵Phi*R

rotation_mat=varimax(A)#调用方差最大旋转函数
rotation_mat=pd.DataFrame(rotation_mat)#数据框化
```

In [161]:

```
rotation_mat
```

Out[161]:

|      |                         0 |                         1 |                         2 |
| :--- | ------------------------: | ------------------------: | ------------------------: |
| 0    |  (0.21323966276777256+0j) |  (0.49435123401088754+0j) |   (0.8072644758263128+0j) |
| 1    |   (-0.861602627623943+0j) |  (-0.3811032744161447+0j) |    (0.327451471162522+0j) |
| 2    |   (0.5187267141393943+0j) |  (-0.6888237247452673+0j) |  (0.21354618650891108+0j) |
| 3    |   (0.9090551329962969+0j) |    (-0.19106991871067+0j) |   (0.3233672577963704+0j) |
| 4    | (-0.13785145883578895+0j) |   (0.4329888308401448+0j) |  (-0.8712574657322602+0j) |
| 5    |  (0.00308412468592269+0j) |  (-0.9638776743736194+0j) |   (0.1614485830909371+0j) |
| 6    |   (0.5434813068461372+0j) |   (0.5485709862761546+0j) |  (0.22316460245920597+0j) |
| 7    | (0.004286980412996384+0j) |  (-0.5800754028893825+0j) |  (0.44210027220828235+0j) |
| 8    |   (-0.171684278987763+0j) |  (-0.9797037284493241+0j) | (-0.07663505527980378+0j) |
| 9    |   (0.7652785107076661+0j) | (0.052660107008933976+0j) |   (0.6202246078943493+0j) |

### 9. 计算因子得分.

In [162]:

```
data=np.mat(data) #矩阵化处理

factor_score=(data).dot(A) #计算因子得分

factor_score=pd.DataFrame(factor_score)#数据框化

factor_score.columns=['因子A','因子B','因子C'] #对因子变量进行命名
factor_score
#factor_score.to_excel(outputfile)#打印输出因子得分矩阵
```

Out[162]:

|      |                    因子A |                    因子B |                     因子C |
| :--- | -----------------------: | -----------------------: | ------------------------: |
| 0    |  (-4.148852018585423+0j) |  (3.7218173860420958+0j) |   (1.0268404260474102+0j) |
| 1    | (-3.0692735299739105+0j) |  (-5.775657145949657+0j) | (-0.08291739333905472+0j) |
| 2    | (0.09261106423291608+0j) |   (2.937792806185489+0j) |  (-1.8684628565463521+0j) |
| 3    |   (3.967516384935492+0j) | (0.28862584652034023+0j) |    (1.555317728379365+0j) |
| 4    |   (3.157998099390924+0j) | (-1.1725788927982674+0j) |  (-0.6307779045413688+0j) |

必看机器学习术语:https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#top_of_page

